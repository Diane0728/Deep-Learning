{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåä Kaggle Setup & Run Instructions\n",
                "\n",
                "## 1. Create a New Notebook\n",
                "1. Go to [Kaggle Kernels](https://www.kaggle.com/kernels).\n",
                "2. Click **\"New Notebook\"**.\n",
                "3. **IMPORTANT:** In the right sidebar, open **\"Notebook Options\"** (or \"Session Options\").\n",
                "    *   **Accelerator:** Select **GPU P100** (or T4 x2).\n",
                "    *   **Internet:** Toggle **\"Internet On\"** (Required for maps).\n",
                "\n",
                "## 2. Upload Your Dataset (The `.npy` Files)\n",
                "1.  Look at the **Right Sidebar** (or top menu).\n",
                "2.  Click **\"Add Data\"** (or the **+** icon).\n",
                "3.  Click **\"Upload\"** -> **\"Browse Files\"**.\n",
                "4.  Select ALL your `.npy` files.\n",
                "5.  Give the dataset a title (e.g., \"WaterLevelData\") and click **Create**.\n",
                "\n",
                "## 3. Run the Training\n",
                "1.  **Configure Mode:** at the top of the code, uncomment the mode you want.\n",
                "2.  **Click \"Save Version\"** (Top Right Corner).\n",
                "3.  Select **\"Save & Run All (Commit)\"**.\n",
                "4.  This runs the notebook in the background.\n",
                "\n",
                "## 4. Get Your Results\n",
                "Once the run finishes, check the **Output** section and download:\n",
                "1. `best_model.pth`\n",
                "2. `loss_curve.png`\n",
                "3. `station_maps.png`\n",
                "4. `tidal_cycle.png`\n",
                "5. `KAGGLE_REPORT.md`\n",
                "\n",
                "---\n",
                "\n",
                "# üìä Kaggle Notebook Report: Water Level Prediction\n",
                "\n",
                "This report explains what this notebook does, how to verify it worked, and what results to expect.\n",
                "\n",
                "## 1. Evidence of Training & Final RMSE\n",
                "I have added a new cell at the very end of the notebook to generate this proof.\n",
                "\n",
                "### üìâ A. Visual Evidence (Loss Curve)\n",
                "- **Cell Name:** \"Plot Training Evidence\" (Last Cell)\n",
                "- **What it does:** Generates a graph (`loss_curve.png`) showing:\n",
                "    - **Blue Line:** Training Loss (MSE).\n",
                "    - **Orange Line:** Validation **RMSE**.\n",
                "    - **Red Dashed Line:** The **Persistence Baseline (0.0859)**.\n",
                "- **Success Criteria:** The Orange line (Model Error) must go **BELOW** the Red line (Baseline).\n",
                "\n",
                "### üî¢ B. Numerical Evidence (Final RMSE)\n",
                "- **Where to find it:**\n",
                "    - Look at the **printed logs** of the Training Loop (Cell 11).\n",
                "    - Look for the line: `üèÜ FINAL BEST VALIDATION RMSE: 0.XXXXX` in the last cell.\n",
                "    - This number contains the **RMSE**, so you can directly compare it to **0.0859**.\n",
                "\n",
                "## 2. Expected Results by Run Mode\n",
                "The **Persistence Baseline** is **CONSTANT (0.0859)** for ALL modes because the Test Set (Year 2020) is always the same.\n",
                "\n",
                "### üöÄ Mode 1: Quick Test (0.1% Data, 5 Epochs)\n",
                "- **Purpose:** Verify code runs without crashing.\n",
                "- **Expected RMSE:** High (~0.2500 - 0.4000).\n",
                "- **Outcome:** WORSE than baseline (normal).\n",
                "\n",
                "### ‚ö° Mode 2: Good Results (1% Data, 20 Epochs)\n",
                "- **Purpose:** Check learning behavior fast.\n",
                "- **Expected RMSE:** Okay (~0.0900 - 0.1500).\n",
                "- **Outcome:** Close to baseline.\n",
                "\n",
                "### üèÜ Mode 3: Best Results (50%-100% Data, 40 Epochs)\n",
                "- **Purpose:** Final submission model.\n",
                "- **Expected RMSE:** Excellent (< 0.0859).\n",
                "- **Outcome:** BEATS the baseline clearly."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Configuration (Select One Mode)\n",
                "**Process:** Choose how much data to use and how long to train.\n",
                "**Explanation:** \n",
                "- `DATA_FRACTION`: Controls the percentage of the dataset used (0.1% to 100%).\n",
                "- `NUM_EPOCHS`: Controls how many times the model sees the data.\n",
                "- **Run ONLY ONE of the cells below** depending on if you are testing or doing the final run."
            ]
        },
import gdown
!gdown 1Ncexf_vB55cpiCeNr-hIRrdpquYaav6B
!gdown 1THbGvO9mVjg_wfZTabRbQBOVpaECl3my
!gdown 16M1zB54PKkKS6SK8W_U83UURWu1T_AxR
!gdown 161OYs8KQSn3RrXezCNwFvOewXLJe5wBf
!gdown 1iwqd4xzHc98OYqBpGsuUW4SG5AQDtdNR #(8784, 5000)  hours,nodes water levels
!gdown 1cHqyeXtmaiC_3v9uadMD7Y0hONGf2R1q
!gdown 1AoFAD2viMarikhU5b5Etdsklx08EzKKb
!gdown 1sWoTlJih-mqDdP9TBzhyXTqHHS5Jr9fe
!gdown 1Mg52QAIo4bfpzJF0dsI8mpZf09tHTzj8
!gdown 1wWz0EWbGiBkZ0vfJD8KeZkmVZfmRiYLk
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MODE 1: QUICK TEST (<5 mins)\n",
                "# Use this to verify code works before long run\n",
                "DATA_FRACTION = 0.001     # 0.1% Data\n",
                "MAX_SAMPLES = 100000\n",
                "NUM_EPOCHS = 5\n",
                "BATCH_SIZE = 2048\n",
                "Est_Time = \"~2-5 mins\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MODE 2: GOOD RESULTS (~30 mins)\n",
                "# decent accuracy, fast turnaround\n",
                "DATA_FRACTION = 0.01      # 1% Data\n",
                "MAX_SAMPLES = 1000000\n",
                "NUM_EPOCHS = 20\n",
                "BATCH_SIZE = 2048\n",
                "Est_Time = \"~30 mins\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MODE 3: BEST RESULTS (>10 Hours)\n",
                "# Maximum accuracy for final submission\n",
                "DATA_FRACTION = 0.5       # 50% Data (Use 1.0 if RAM allows)\n",
                "MAX_SAMPLES = 4000000\n",
                "NUM_EPOCHS = 40\n",
                "BATCH_SIZE = 2048\n",
                "Est_Time = \"~10-12 Hours\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"üöÄ SELECTED MODE: {Est_Time} estimated runtime.\")\n",
                "print(f\"   Fraction: {DATA_FRACTION*100}%\")\n",
                "print(f\"   Epochs: {NUM_EPOCHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Setup & Imports\n",
                "**Process:** Load necessary libraries and check hardware.\n",
                "**Explanation:**\n",
                "- Imports PyTorch (neural networks), NumPy (math), Matplotlib (plotting).\n",
                "- Also writes the **`KAGGLE_REPORT.md`** file so you can download it later.\n",
                "- Checks if a GPU is available (`Using: cuda`) for faster training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from tqdm.notebook import tqdm\n",
                "import os\n",
                "import gc\n",
                "import time\n",
                "from datetime import datetime\n",
                "\n",
                "# GENERATE REPORT FILE\n",
                "# This creates 'KAGGLE_REPORT.md' in the Output section for download\n",
                "report_content = \"\"\"# üìä Kaggle Notebook Report: Water Level Prediction\n",
                "\n",
                "This report explains what this notebook does, how to verify it worked, and what results to expect.\n",
                "\n",
                "## 1. Evidence of Training & Final RMSE\n",
                "\n",
                "I have added a new cell at the very end of the notebook to generate this proof.\n",
                "\n",
                "### üìâ A. Visual Evidence (Loss Curve)\n",
                "- **Cell Name:** \"Plot Training Evidence\" (Last Cell)\n",
                "- **What it does:** Generates a graph (`loss_curve.png`) showing:\n",
                "    - **Blue Line:** Training Loss (MSE).\n",
                "    - **Orange Line:** Validation **RMSE**.\n",
                "    - **Red Dashed Line:** The **Persistence Baseline (0.0859)**.\n",
                "- **Success Criteria:** The Orange line (Model Error) must go **BELOW** the Red line (Baseline).\n",
                "\n",
                "### üî¢ B. Numerical Evidence (Final RMSE)\n",
                "- **Where to find it:**\n",
                "    - Look at the **printed logs** of the Training Loop (Cell 11).\n",
                "    - Look for the line: `üèÜ FINAL BEST VALIDATION RMSE: 0.XXXXX` in the last cell.\n",
                "    - This number contains the **RMSE**, so you can directly compare it to **0.0859**.\n",
                "\n",
                "## 2. Expected Results by Run Mode\n",
                "\n",
                "The **Persistence Baseline** is **CONSTANT (0.0859)** for ALL modes because the Test Set (Year 2020) is always the same.\n",
                "\n",
                "### üöÄ Mode 1: Quick Test (0.1% Data, 5 Epochs)\n",
                "- **Purpose:** Verify code runs without crashing.\n",
                "- **Expected RMSE:** High (~0.2500 - 0.4000).\n",
                "- **Outcome:** WORSE than baseline (normal).\n",
                "\n",
                "### ‚ö° Mode 2: Good Results (1% Data, 20 Epochs)\n",
                "- **Purpose:** Check learning behavior fast.\n",
                "- **Expected RMSE:** Okay (~0.0900 - 0.1500).\n",
                "- **Outcome:** Close to baseline.\n",
                "\n",
                "### üèÜ Mode 3: Best Results (50%-100% Data, 40 Epochs)\n",
                "- **Purpose:** Final submission model.\n",
                "- **Expected RMSE:** Excellent (< 0.0859).\n",
                "- **Outcome:** BEATS the baseline clearly.\n",
                "\n",
                "---\n",
                "\n",
                "## 3. Cell-by-Cell Explanations\n",
                "\n",
                "### Step 1: Configuration\n",
                "**Process:** Choose how much data to use and how long to train.\n",
                "\n",
                "### Step 2: Setup & Imports\n",
                "**Process:** Load necessary libraries and check hardware.\n",
                "\n",
                "### Step 3: Load Data Files\n",
                "**Process:** Find and load the `.npy` files from the Kaggle input directory.\n",
                "\n",
                "### Step 4: Preprocessing & Baseline\n",
                "**Process:** Normalize the data and calculate the persistence baseline.\n",
                "\n",
                "### Step 5: Visualization - Station Locations (Train vs Test)\n",
                "**Process:** Create separate maps for train and test stations.\n",
                "**Explanation:** Displays the static locations of the sensors in the Adriatic Sea. Note that the locations are the same for both datasets.\n",
                "\n",
                "### Step 6: Visualization - Tidal Cycles (Single Node)\n",
                "**Process:** Plot the water level for one station over 200 hours.\n",
                "**Explanation:** Visualizes the 24-hour tidal cycle pattern found in the data.\n",
                "\n",
                "### Step 7: Create Dataset Class\n",
                "**Process:** Define how PyTorch reads the data in batches.\n",
                "\n",
                "### Step 8: Define Neural Network (BiLSTM)\n",
                "**Process:** Create the deep learning model architecture.\n",
                "\n",
                "### Step 9: Training Loop\n",
                "**Process:** Train the model iteratively and save the best version.\n",
                "\n",
                "### Step 10: Visualization - Loss Curves (Evidence)\n",
                "**Process:** Plot the training progress and compare with baseline.\n",
                "\"\"\"\n",
                "\n",
                "with open(\"KAGGLE_REPORT.md\", \"w\") as f:\n",
                "    f.write(report_content)\n",
                "print(\"‚úÖ Generated KAGGLE_REPORT.md (Check Output Section to Download)\")\n",
                "\n",
                "# Cartopy installation check (Kaggle usually needs this)\n",
                "try:\n",
                "    import cartopy.crs as ccrs\n",
                "    import cartopy.feature as cfeature\n",
                "    import cartopy.io.img_tiles as cimgt\n",
                "    print(\"‚úÖ Cartopy available.\")\n",
                "except ImportError:\n",
                "    print(\"‚ö†Ô∏è Installing Cartopy...\")\n",
                "    !pip install -q cartopy\n",
                "    import cartopy.crs as ccrs\n",
                "    import cartopy.feature as cfeature\n",
                "    import cartopy.io.img_tiles as cimgt\n",
                "\n",
                "# GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load Data Files\n",
                "**Process:** Find and load the `.npy` files from the Kaggle input directory.\n",
                "**Explanation:**\n",
                "- `find_file_path()`: Automatically searches for your files, so you don't need to worry about exact paths.\n",
                "- Loads Training Data (2010-2019) and Test Data (2020).\n",
                "- Verifies if any files are missing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "\n",
                "# SMART FILE FINDER\n",
                "# This function searches all subfolders in /kaggle/input to find your files\n",
                "def find_file_path(filename):\n",
                "    search_roots = ['/kaggle/input', '.', '../input']\n",
                "    for root in search_roots:\n",
                "        for dirpath, dirnames, filenames in os.walk(root):\n",
                "            if filename in filenames:\n",
                "                return os.path.join(dirpath, filename)\n",
                "    return None\n",
                "\n",
                "required = ['wl_2010-2019.npy', 'dist_alt_az_moon-sun_coord13-45_2010-2019_norm.npy', \n",
                "            'ERA5_adriatic_u10v10sp_2010-2019.npy', 'wl_2020.npy', \n",
                "            'dist_alt_az_moon-sun_coord13-45_2020_norm.npy', 'ERA5_adriatic_u10v10sp_2020.npy',\n",
                "            'lat.npy', 'lon.npy']\n",
                "\n",
                "data = {}\n",
                "missing = []\n",
                "\n",
                "print(\"üîç Searching for files...\")\n",
                "for f in required:\n",
                "    path = find_file_path(f)\n",
                "    if path:\n",
                "        print(f\"  Found {f} at: {path}\")\n",
                "        # Load logic\n",
                "        if 'dist_alt' in f:\n",
                "            data[f] = np.load(path).T\n",
                "        elif 'ERA5' in f:\n",
                "            data[f] = np.transpose(np.load(path), (1, 0, 2, 3))\n",
                "        else:\n",
                "            data[f] = np.load(path)\n",
                "    else:\n",
                "        missing.append(f)\n",
                "\n",
                "if missing:\n",
                "    print(f\"\\n‚ùå STILL MISSING: {missing}\")\n",
                "    print(\"üëâ Did you click 'Add Data' on the right sidebar and upload them?\")\n",
                "else:\n",
                "    print(\"\\n‚úÖ All data loaded successfully!\")\n",
                "    train_wl = data['wl_2010-2019.npy']\n",
                "    train_eph = data['dist_alt_az_moon-sun_coord13-45_2010-2019_norm.npy']\n",
                "    train_era5 = data['ERA5_adriatic_u10v10sp_2010-2019.npy']\n",
                "    test_wl = data['wl_2020.npy']\n",
                "    test_eph = data['dist_alt_az_moon-sun_coord13-45_2020_norm.npy']\n",
                "    test_era5 = data['ERA5_adriatic_u10v10sp_2020.npy']\n",
                "    lat_vec = data['lat.npy']\n",
                "    lon_vec = data['lon.npy']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Preprocessing & Baseline\n",
                "**Process:** Normalize the data and calculate the persistence baseline.\n",
                "**Explanation:**\n",
                "- `RMSE`: Root Mean Squared Error (our main success metric).\n",
                "- **Baseline:** Calculates the error of a naive model (predicting tomorrow = today). Value should be ~`0.0859`. \n",
                "- **Normalization:** scales data to mean=0 and std=1, which helps neural networks learn faster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_era5_coord(lat, lon):\n",
                "    \"\"\"Function to get era5 data from data coordinates\"\"\"\n",
                "    era5_row, era5_col = 5, 9\n",
                "    lat_min, lat_max = 44.94972, 45.8\n",
                "    lon_min, lon_max = 12.12863, 13.81283\n",
                "    delta_lat, delta_lon = lat_max - lat_min, lon_max - lon_min\n",
                "    lon_coord = np.ceil((lon - lon_min) / delta_lon * (era5_col -1))\n",
                "    lat_coord = 4 - np.ceil((lat - lat_min) / delta_lat * (era5_row - 1))\n",
                "    return int(lat_coord), int(lon_coord)\n",
                "\n",
                "def RMSE(wl_true, wl_pred):\n",
                "    return np.sqrt(np.mean(np.square(wl_pred - wl_true)))\n",
                "\n",
                "# Calc Baseline\n",
                "baseline = RMSE(test_wl[:-1], test_wl[1:])\n",
                "print(f\"Persistence Baseline: {baseline:.4f}\")\n",
                "\n",
                "# Pre-calc ERA5 indices for all nodes\n",
                "era5_indices = np.array([get_era5_coord(l, o) for l, o in zip(lat_vec, lon_vec)])\n",
                "\n",
                "# Normalize\n",
                "scaler_era5 = StandardScaler()\n",
                "train_era5_norm = scaler_era5.fit_transform(train_era5.reshape(train_era5.shape[0], -1)).reshape(train_era5.shape)\n",
                "test_era5_norm = scaler_era5.transform(test_era5.reshape(test_era5.shape[0], -1)).reshape(test_era5.shape)\n",
                "\n",
                "scaler_wl = StandardScaler()\n",
                "train_wl_norm = scaler_wl.fit_transform(train_wl)\n",
                "test_wl_norm = scaler_wl.transform(test_wl)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Visualization - Station Locations (Train vs Test)\n",
                "**Process:** Create separate maps for train and test stations.\n",
                "**Explanation:**\n",
                "- Plots the latitude and longitude of sensors.\n",
                "- **Note:** They are the same stations (red dots) for both 2010-2019 (Train) and 2020 (Test)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üó∫Ô∏è Generating Station Location Maps...\")\n",
                "fig, axes = plt.subplots(1, 2, figsize=(20, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
                "\n",
                "(lat_min, lat_max), (lon_min, lon_max) = (lat_vec.min(), lat_vec.max()), (lon_vec.min(), lon_vec.max())\n",
                "\n",
                "titles = ['Train Set Nodes (2010-2019)', 'Test Set Nodes (2020)']\n",
                "for ax, title in zip(axes, titles):\n",
                "    ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
                "    try:\n",
                "        stamen_terrain = cimgt.GoogleTiles(style='satellite')\n",
                "        ax.add_image(stamen_terrain, 10)\n",
                "    except:\n",
                "        ax.stock_img()\n",
                "    ax.add_feature(cfeature.RIVERS, edgecolor='cyan', linewidth=0.5)\n",
                "    ax.add_feature(cfeature.BORDERS, linestyle='-', edgecolor='white', alpha=0.5)\n",
                "    # Plot Nodes\n",
                "    ax.scatter(lon_vec, lat_vec, cmap=\"viridis\", c=\"r\", s=10, marker='o', transform=ccrs.PlateCarree())\n",
                "    ax.set_title(title)\n",
                "\n",
                "plt.savefig(\"station_maps.png\", dpi=300)\n",
                "plt.show()\n",
                "print(\"‚úÖ Saved station_maps.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Visualization - Tidal Cycles (Single Node)\n",
                "**Process:** Plot the water level for one station over 200 hours.\n",
                "**Explanation:**\n",
                "- Shows the 24-hour tidal cycle.\n",
                "- Uses `test_wl[:200, 0]` (First 200 hours of the first station)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üåä Generating Tidal Cycle Plot...\")\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.plot(test_wl[:200, 0], label='Water Level (Station 0)')\n",
                "plt.title('Single Node Variability (First 200 Hours)')\n",
                "plt.xlabel('Time (Hours)')\n",
                "plt.ylabel('Water Level (m)')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig(\"tidal_cycle.png\", dpi=300)\n",
                "plt.show()\n",
                "print(\"‚úÖ Saved tidal_cycle.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Create Dataset Class\n",
                "**Process:** Define how PyTorch reads the data in batches.\n",
                "**Explanation:**\n",
                "- `WaterLevelDataset`: Converts the raw arrays into PyTorch Tensors.\n",
                "- `DataLoader`: Handles shuffling and batching (process 2048 timestamps at a time).\n",
                "- Uses `DATA_FRACTION` to limit the data size if in 'Quick Test' mode."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset Class\n",
                "class WaterLevelDataset(Dataset):\n",
                "    def __init__(self, wl_norm, ephemerides, era5_norm, lat_vec, lon_vec, era5_indices, seq_len=72, mode='train'):\n",
                "        self.eph = ephemerides\n",
                "        self.era5_norm = era5_norm\n",
                "        self.lat_vec, self.lon_vec = lat_vec, lon_vec\n",
                "        self.era5_indices = era5_indices\n",
                "        self.seq_len = seq_len\n",
                "        self.n_nodes = len(lat_vec)\n",
                "        self.wl_norm = wl_norm\n",
                "        self.n_timesteps = len(wl_norm)\n",
                "        self.total_samples = (self.n_timesteps - seq_len) * self.n_nodes\n",
                "        \n",
                "        if mode == 'train':\n",
                "            self.n_use = min(int(self.total_samples * DATA_FRACTION), MAX_SAMPLES)\n",
                "            np.random.seed(42)\n",
                "            self.indices = np.random.choice(self.total_samples, self.n_use, replace=False)\n",
                "        else:\n",
                "            # Test set sample for quick validation during training (full eval done at end)\n",
                "            self.n_use = min(self.total_samples, 1000000)\n",
                "            np.random.seed(42)\n",
                "            self.indices = np.random.choice(self.total_samples, self.n_use, replace=False)\n",
                "\n",
                "    def __len__(self): return len(self.indices)\n",
                "    def __getitem__(self, idx):\n",
                "        flat_idx = self.indices[idx]\n",
                "        n_idx = flat_idx % self.n_nodes\n",
                "        t_idx = (flat_idx // self.n_nodes) + self.seq_len\n",
                "        \n",
                "        e_i, e_j = self.era5_indices[n_idx]\n",
                "        s_start = t_idx - self.seq_len\n",
                "        \n",
                "        return (torch.FloatTensor(self.eph[s_start:t_idx]), \n",
                "                torch.FloatTensor(self.era5_norm[s_start:t_idx, :, e_i, e_j]), \n",
                "                torch.FloatTensor([self.lat_vec[n_idx], self.lon_vec[n_idx]]), \n",
                "                torch.FloatTensor([self.wl_norm[t_idx, n_idx]]))\n",
                "\n",
                "SEQ_LEN = 72\n",
                "train_set = WaterLevelDataset(train_wl_norm, train_eph, train_era5_norm, lat_vec, lon_vec, era5_indices, SEQ_LEN, 'train')\n",
                "test_set = WaterLevelDataset(test_wl_norm, test_eph, test_era5_norm, lat_vec, lon_vec, era5_indices, SEQ_LEN, 'test')\n",
                "\n",
                "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
                "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
                "print(\"‚úÖ DataLoaders Ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Define Neural Network (BiLSTM)\n",
                "**Process:** Create the deep learning model architecture.\n",
                "**Explanation:**\n",
                "- **BiLSTM:** Bi-directional Long Short-Term Memory. Good for time-series.\n",
                "- **Output:** Predicts the water level for the next timestamp."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class WaterLevelLSTM(nn.Module):\n",
                "    def __init__(self, n_eph, hidden_size, num_layers, dropout):\n",
                "        super().__init__()\n",
                "        self.lstm = nn.LSTM(n_eph+3, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
                "        self.ln = nn.LayerNorm(hidden_size)\n",
                "        self.attn = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1))\n",
                "        self.coord_net = nn.Sequential(nn.Linear(2, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, 64), nn.ReLU())\n",
                "        self.predictor = nn.Sequential(nn.Linear(hidden_size+64, 256), nn.ReLU(), nn.BatchNorm1d(256), nn.Dropout(dropout), \n",
                "                                       nn.Linear(256, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, 1))\n",
                "    def forward(self, eph, era5, coords):\n",
                "        x = torch.cat([eph, era5], dim=-1)\n",
                "        lstm_out, _ = self.lstm(x)\n",
                "        context = (self.ln(lstm_out) * torch.softmax(self.attn(self.ln(lstm_out)), dim=1)).sum(dim=1)\n",
                "        return self.predictor(torch.cat([context, self.coord_net(coords)], dim=-1))\n",
                "\n",
                "HIDDEN_SIZE = 256\n",
                "NUM_LAYERS = 2\n",
                "DROPOUT = 0.25\n",
                "LEARNING_RATE = 0.0003\n",
                "\n",
                "model = WaterLevelLSTM(train_eph.shape[1], HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
                "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
                "criterion = nn.MSELoss()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Training Loop\n",
                "**Process:** Train the model iteratively and save the best version.\n",
                "**Explanation:**\n",
                "- Supports resuming from `checkpoint.pth` if interrupted.\n",
                "- **Training:** Updates model weights to minimize error.\n",
                "- **Validation:** Calculates **RMSE** on test data after every epoch.\n",
                "- **Saving:** Saves `best_model.pth` ONLY when `Val RMSE < Best RMSE`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHECKPOINT_PATH = 'checkpoint.pth'\n",
                "best_val_loss = float('inf')\n",
                "train_losses, val_losses = [], []\n",
                "\n",
                "if os.path.exists(CHECKPOINT_PATH):\n",
                "    print(\"üîÑ Resuming from Checkpoint...\")\n",
                "    # FIX for UnpicklingError in newer PyTorch versions\n",
                "    cp = torch.load(CHECKPOINT_PATH, weights_only=False)\n",
                "    model.load_state_dict(cp['model_state_dict'])\n",
                "    optimizer.load_state_dict(cp['optimizer_state_dict'])\n",
                "    start_epoch = cp['epoch'] + 1\n",
                "    best_val_loss = cp['best_val_loss']\n",
                "    train_losses = cp.get('train_losses', [])\n",
                "    val_losses = cp.get('val_losses', [])\n",
                "else:\n",
                "    start_epoch = 0\n",
                "\n",
                "print(f\"üèÅ Starting Training: {NUM_EPOCHS} Epochs\")\n",
                "for epoch in range(start_epoch, NUM_EPOCHS):\n",
                "    t0 = time.time()\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
                "    for e, w, c, y in pbar:\n",
                "        e, w, c, y = e.to(device), w.to(device), c.to(device), y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(e, w, c)\n",
                "        loss = criterion(out, y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    avg_train_loss = total_loss / len(train_loader)\n",
                "    \n",
                "    # Validate (Calculate RMSE)\n",
                "    model.eval()\n",
                "    v_mse = 0\n",
                "    with torch.no_grad():\n",
                "        for e, w, c, y in test_loader:\n",
                "            # criterion is MSELoss, so this adds Mean Squared Error\n",
                "            v_mse += criterion(model(e.to(device), w.to(device), c.to(device)), y.to(device)).item()\n",
                "    \n",
                "    avg_val_mse = v_mse / len(test_loader)\n",
                "    avg_val_rmse = np.sqrt(avg_val_mse)  # Convert MSE to RMSE\n",
                "    \n",
                "    scheduler.step()\n",
                "    train_losses.append(avg_train_loss)\n",
                "    val_losses.append(avg_val_rmse)      # Store RMSE for plotting\n",
                "    \n",
                "    dt = (time.time() - t0) / 60\n",
                "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.5f} | Val RMSE: {avg_val_rmse:.5f} | Best RMSE: {best_val_loss:.5f} | Time: {dt:.1f}m\")\n",
                "    \n",
                "    # Save Checkpoint\n",
                "    torch.save({\n",
                "        'epoch': epoch,\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict(),\n",
                "        'scheduler_state_dict': scheduler.state_dict(),\n",
                "        'best_val_loss': best_val_loss,\n",
                "        'train_losses': train_losses,\n",
                "        'val_losses': val_losses\n",
                "    }, CHECKPOINT_PATH)\n",
                "    \n",
                "    # Save Best Model (Compare RMSE)\n",
                "    if avg_val_rmse < best_val_loss:\n",
                "        best_val_loss = avg_val_rmse\n",
                "        torch.save(model.state_dict(), 'best_model.pth')\n",
                "        print(f\"  üíæ New Best Model (RMSE: {best_val_loss:.5f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Visualization - Loss Curves (Evidence)\n",
                "**Process:** Plot the training progress and compare with baseline.\n",
                "**Explanation:**\n",
                "- **Blue Line (Train):** Should go down steadily.\n",
                "- **Orange Line (Val RMSE):** This is the MOST IMPORTANT line. It must go below the Red Line.\n",
                "- **Red Line (Baseline):** Constant at `0.0859`. If Orange < Red, the model works!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Training Evidence\n",
                "print(\"üìà Generating Loss Curves...\")\n",
                "if len(train_losses) > 0:\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
                "    plt.plot(val_losses, label='Validation RMSE', linewidth=2)\n",
                "    plt.axhline(y=baseline, color='r', linestyle='--', label=f'Baseline ({baseline:.4f})')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('RMSE / Loss')\n",
                "    plt.title('Training Evidence: Loss Curve')\n",
                "    plt.legend()\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.savefig(\"loss_curve.png\", dpi=300)\n",
                "    plt.show()\n",
                "    print(\"‚úÖ Saved loss_curve.png\")\n",
                "    print(f\"üèÜ FINAL BEST VALIDATION RMSE: {best_val_loss:.5f}\")\n",
                "    if best_val_loss < baseline:\n",
                "        print(\"üéâ SUCCESS: Model beat the baseline!\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è Model did not beat baseline yet. Try running more epochs or Mode 3.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No training data found to plot yet.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
